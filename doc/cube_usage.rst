=========================================
How can I use the Earth System Data Cube?
=========================================

Technical Dataset Description
=============================

.. Responsible: BC


General Data Cube Format, Content and Organisation
--------------------------------------------------

The binary data format for the Earth System Data Cube (ESDC) in the CAB-LAB project is **netCDF 4 classic**, where the term classic stands for an
underlying HDF-5 format accessed by a netCDF 4 API.

The netCDF file's content and structure follows the `CF-conventions <http://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html>`_.
That is, there are are always at least three dimensions defined

1. ``lon`` - Always the inner and therefore fastest varying dimension. Defines the **raster width** of spatial images.
2. ``lat`` - Always the second dimension. Defines the **raster height** of spatial images.
3. ``time`` - Time dimension.

There are 1D-variables related to each dimension providing its actual values:

* ``lon(lon)`` and ``lat(lon)`` - longitudes and latitudes in *decimal degrees* defined in a WGS-84 geographical
  coordinate reference system. The spatial grid is homogeneous with the distance between two grid points referred to as
  the Data Cube's **spatial resolution**.
* ``start_time(time)`` and ``end_time(time)`` - Period start and end times given in *days since 2001-01-01 00:00*.
  The increments between two vlaues in time are identical and referred to as the Data Cube's **temporal resolution**.

.. todo:: Norman, you never refer to start_time or end_time after this, just to time. Confusing without any further information. What exactly are start and stop times?
    Also, i guess one wold rather name it start and end as stop implies some action.

There is usually only a single geophysical variable with *shape*\ (``time``, ``lat``, ``lon``) represented by each
netCDF file. So each netCDF file is composed of *length*\ (``time``) spatial images of that variable, where each image
of size *length*\ (``lon``) x *length*\ (``lat``) pixels has been generated by aggregating all source data contributing
to the period given by the Data Cube's temporal resolution.

To limit the size of individual files, the geophysical variables are stored in one file per year. For example,
if the temporal resolution is 0.25 degrees and the the spatial resolution is 8-day periods then there will be up to 46
images of 1440 x 720 pixels in each annual netCDF file. These annual files are stored in dedicated sub-directories
as follows::

    <cube-root-dir>/
        cube.config
        data/
            LAI/
                2001_LAI.nc
                2002_LAI.nc
                ...
                2011_LAI.nc
            Ozone/
                2001_Ozone.nc
                2002_Ozone.nc
                ...
                2011_Ozone.nc
            ...

The names of the geophysical variable in a netCDF file must match the name of its corresponding sub-directory and the
names of the their contained files.

.. todo:: Norman, CF conventions (and COARDS) are rather strict with respect to naming of variables. Check: http://cfconventions.org/Data/cf-standard-names/30/build/cf-standard-name-table.html

The text file ``cube.config`` contains a Data Cube's static configuration such as its temporal and spatial resolution.
Also the spatial coverage is constant, that is, all spatial images are of the same size. Where actual data is missing,
fill values are inserted to expand a data set to the dimensions of the Data Cube.
The fill values in the Data Cube are identical to the ones used in the Data Cube's sources. The same holds for the data types.
While all images for all time periods have the same size, the temporal coverage for a given variable may vary.
Missing spatial images for a given time period are treated as images with all pixels set to a fill value.

The following table contains all possible configuration parameters:

====================  ==============================  ==========================================================
Parameter             Default Value                   Description
====================  ==============================  ==========================================================
``temporal_res``      ``8``                           The constant temporal resolution given as integer days.
``calendar``          ``'gregorian'``                 Defines this Cube's time units.
``ref_time``          ``datetime(2001, 1, 1)``        The Cube's time units are days since this reference date/time.
``start_time``        ``datetime(2001, 1, 1)``        The start date/time of contributing source data.
``end_time``          ``datetime(2011, 1, 1)``        The end date/time of contributing source data.
``spatial_res``       ``0.25``                        The constant spatial resolution given in decimal degrees.
``grid_x0``           ``0``                           The spatial grid's X-offset. *Not used yet.*
``grid_y0``           ``0``                           The spatial grid's Y-offset. *Not used yet.*
``grid_width``        ``1440``                        The spatial grid's width. Must always be 360 / ``spatial_res``.
``grid_height``       ``720``                         The spatial grid's height. Must always be 180 / ``spatial_res``.
``variables``         ``None``                        The variables contained in this Cube.  *Not used yet.*
``file_format``       ``'NETCDF4_CLASSIC'``           The target binary file format.
``compression``       ``False``                       Whether or not the target binary files should be compressed.
``model_version``     ``'0.1'``                       The version of the Data Cube model and configuration.
====================  ==============================  ==========================================================

General Processing Methods Description
--------------------------------------

The Data Cube is generated by the ``cube-cli`` tool. This tools creates a Data Cube for a given configuration
and can be used to subsequently add variables, one by one, to the Cube. Each variable is read from its specific data source and
transformed in time and space to comply to the specification defined by the target Cube's configuration.

The general approach is as follows: For each variable and a given Cube time period:
* Read the variable's data from all contributing sources that have an overlap with the target period;
* Perform temporal aggregation of all contributing spatial images in the original spatial resolution;
* Perform spatial upsampling or downsampling of the image aggregated in time;
* Mask the resulting upsampled/downsampled image by the common land-sea mask;
* Insert the final image for the variable and target time period into the Data Cube.

.. todo:: Fabian: provide scientific justification here for this approach.*

The following sections describe each method used in more detail.

Gap-Filling Approach
####################

The current version (version 0.1, Nov 2015) of the ESDC does not explicitly fill gaps. However, some
gap-filling occurs during temporal aggregation as described below. The CAB-LAB team may provide
gap-filled ESDC versions at a later point in time of the project. Gap-filling is part of the *Data Analytics
Toolkit* and is thus not tackled during Cube generation to retain the information on the original data coverage
as much as possible.

For future Cube versions per-variable gap-filling strategies may be applied. Also, only a spatio-temporal
region of interest may be gap-filled while cells outside this region may be filled by global default values. An instructive example
of such an approach would be the gap-filling of a LAI data set, which only takes place in mid-latitudes while gaps in high-latitudess are
filled with zeros.

.. todo:: Whoever wrote the above should at least reveal the full name of LAI and give a simple explanation why such a gap-filling may make sense at all.
    Moreover, filling gaps with zeros is in fact gap-filling.

Temporal Resampling
###################

Temporal resampling starts on the 1st January of every year so that all the *i*-th spatial images in the ESDC
refer to the same time of the year, namely starting *i* x *temporal resolution*. Source data is collected for every
resulting ESDC target period. If there is more than one contribution in time, then each contribution is weighted
according to the temporal overlap with the target period. Finally, target pixel values are computed by averaging
all weighted values in time not masked by a fill value. By doing so, some temporal gaps are filled implicitly.

.. todo:: Norman: put graphic here showing how weights are determined.*

.. todo:: Norman: put equation here including weights and also respect fill values.*

Spatial Resampling
##################

Spatial resampling occurrs after temporal resampling if the ESDC's spatial
resolution differs from the data source resolution.

If the ESDC's spatial resolution is higher than the data source spatial resolution, source images are **upsampled
by rescaling hereby duplicating original values, but not performing any spatial interpolation**.

If the ESDC's spatial resolution is lower than the data source spatial resolution, source images are **downsampled
by aggregation hereby performing a weighted spatial averaging taking into account missing values**. If there is not an
integer factor between the source and Cube resolution, weights will be found according to the spatial overlap of source
and target cells.

Land-Water Masking
##################

After spatial resampling, a land-water mask is applied to individual variables depending on whether
a variable is defined for water surfaces only, land surfaces only, or both. A common land-water mask is used for all
variables for a given spatial Cube resolution. Masked values are indicated by fill values.



Constraints and Limitations
---------------------------

The Data Cube's approach of transforming all variables onto a common grid greatly facilitates handling and joint analysis
of data sets that originally had different characteristics and were generated under different assumptions.
Regridding, gap-filling, and averaging, however, may alter the information contained in the original data considerably.

The main idea of the ESDC is to provide a consistent and synoptic characterisation of the Earth System at given time steps to promote global analyses.
Therefore, conducting small-scale, high frequency studies that are potentially highly sensible to individual artifacts introduced by data transformation is not
encouraged. The cautious expert user may hence carefully check phenomena close to the Land-Sea mask or in data sparse
regions of the original data. If in doubt, suspicious patterns in the ESDC or unexpected analytical results should be verified with the source data in the native resolution.
We try here as much as possible to conserve the characteristics of the original data, while facilitating data handling and analysis by transformation.

This is a difficult balance to strike that at times involves inconvenient trade-offs. We thus embrace transparency and reproducibility to enable the
informed user to evaluate the validity and consistency of the processed data and strive to offer options for data transformation wherever possible.

.. todo:: Elaborate further! Or at least revise if you feel like it.

Dataset Usage
=============

The standard way of accessing the Data Cube is through the Data Access API. Alternatively, the netcdf files that comprise
 the Data Cube can also be directly read by any other adequate method, e.g. by an implementation of the netcdf library in
 any programming language or suitable viewer software.

Dataset Access Service
----------------------

.. Responsible: BC*

.. todo:: Responsible BC. GB: it is unclear to me what is the difference between this one and the Data access API. Norman, enlighten me!

Data Access API
---------------

.. Responsible: BC

.. todo:: Responsible BC.

.. _DAT:

Data Analytics Toolkit
----------------------

In addition to the Data Access API which enables the user to conveniently read data from the file, we provide a Data Analytics Toolkit (DAT) to facilitate analysis and
visualization of the Data Cube. The DAT is hosted on Github and is developed in close interaction with the scientific community. The CABLAB-consortium is leading and directly
involved in the development of a toolkit for Python and the Julia programming language (https://github.com/CAB-LAB/CABLAB.jl).


Julia implementation of the DAT
-------------------------------

The current implementation of the Julia Data Anaytics toolkit consists mainly of 3 parts:

  1. A collection of time series and spatial analysis functions, that are easily applicable to data imported from the Data Cube.
  2. Functions for visualizing time series and spatial maps of Cube Data.
  3. A macro that lets a user add custom functions to the Toolkit

1. Collection of analysis functions

We provide a list of methods to perform basic statistical analysis on the DataCube in a most user-friendly way. This means that the user will not have to write source code for applying these basic functions but can simply call a function that accepts a DataCube representation as its first argument. Splitting of the data along the appropriate axex, handling of missing data and formatting the rsults are done automatically  by these Toolkit functions. Here we provide a list of analysis methods that are currently implemented in the DAT. Details about their usage can be found in the DAT documentation:

  - **removeMSC** subtracts the mean annual cycle of each time series in the Cube
  - **gapFillMSC** uses the mean annual cycle to fill missinf values per time series in the cube
  - **normalize** normalizes the whole cube so that each time series has zero mean and unit variance
  - **timeVariance** calculates the Variance per time series
  - **timeMean** returns the time mean for each time series in the cube

2. Visualisation of the DataCube

For a convenient and interactive visual inspection of the DataCube there are currently two plotting function available: plotTS for plotting time series and plotMAP for plotting maps of the cube data. The resulting plots are interactive. For example when plotting time series of a 4D dataset, there are sliders that let the user move through longitudes and latitudes as well as buttons where one can switch on and off the different variables. The same holds for plotting maps, where one can move through time steps and switch between variables.

3. Ingesting user-functions into the DAT

It is possible for the user to add, for each individual session custom functions to the DAT. Before we explain how to do this we describe how the DAT handles a Cube's dimensions.
Each cube object has an **axes** property, which is a vector of the cube's axes. Currently the following Axes types are defined:

  - **LonAxis** represents the cube's longitude dimension
  - **LatAxis** represents the cube's latitude dimension
  - **TimeAxis** represents the cube's time dimension
  - **VariableAxis** defines the cube's variables
  - **CountryAxis** can be used for by-country variables

As data is read from the DataCube it has the structure of a 4-dimensional array, having the axes Variable, Time, Latitude and Longitude, so its axes will contain the 4 corresponding axes types.
When one applies a DAT method on a datacube, the resulting datacube might have different dimensions than than the input datacube. For example, if one applies a time average over the cube, the result will be a 3-dimensional cube, the time dimension being eliminated.

A function is added to the DAT using the **@registerDATFunction** macro. Its first argument is a function that is supposed to be applied on slices of the datacube. Its signature must have at least 4 input variables: *xin*, *xout*, which are the input and output data arrays and *maskin* and *maskout* which are the input and output mask. The second argument is a tuple containing the axes the function is going to work on. The third argument is a tuple containing the return axes of the operation. After that additional arguments can be passed that are passed to the underlying function.

To give an example, here is how the Mean seasonal cycle gapfilling function is defined::

    function gapFillMSC(xin::AbstractVector,xout::AbstractVector,maskin::AbstractVector{UInt8},maskout::AbstractVector{UInt8},NpY::Integer)

      msc=getMSC!(xin,xout,maskin,NpY)
      replaceMisswithMSC!(msc,xin,xout,maskin,maskout,NpY)

    end

    @registerDATFunction gapFillMSC (TimeAxis,) (TimeAxis,) NpY::Integer

First we define the atomic function that operates on single time series, which accepts an input and an output vector and mask as its arguments as well as an additional argument which is the number of time steps per year. Then the macro **@registerDATFunction** is called, the input dimension is a TimeAxis as well as the output dimension. Internally this generates a new method of the gapFillMSC function which operates on a Memory representation of a cube object (as returned from a call to getCubeData). It will automatically take care of slicing the data correctly, permute dimensions if necessary and apply the the function and collect the results into an output data structure which is again a memeory representation of a DataCube with the appropriate axes.


Use Cases and Examples
----------------------



.. todo:: Responsible MPI!
    *I would like to include the 2 user strory NoteBooks here. GB have you found a good way to include notebooks? *

Constraints and Limitations
---------------------------

.. todo:: Responsible MPI!
